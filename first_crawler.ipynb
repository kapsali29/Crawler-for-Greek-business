{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το συγκεκριμένο αρχείο κώδικα αποτελεί τον πρώτο Crawler, πιο συγκεκριμένα εντοπίζει τα εξής:\n",
    "\n",
    "1. Κοινωνικά δίκτυα στο website της επιχείρησης\n",
    "2. URL κοινωνικών δικτύων στο website της επιχείρησης\n",
    "3. Αν το website έχει Multi languge option\n",
    "4. Αν το website έχει Search option\n",
    "5. Αν το website έχει Blog\n",
    "6. Αν το website έχει Mobile app\n",
    "7. Αν το website έχει Eshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_crawler(url):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import re\n",
    "    from selenium import webdriver\n",
    "    import time \n",
    "    res = []\n",
    "    results = []\n",
    "    results.append(url)\n",
    "    url,r = check_redirects(url)\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    ##check if domain contains Facebook logo\n",
    "    soc_nets, soc_nets_urls = social_networks(soup)\n",
    "    results.append(soc_nets)\n",
    "    results.append(soc_nets_urls)\n",
    "    ###Multilanguage option\n",
    "    mul_lang = soup.find('a',href=re.compile(r'/en|/usa|/uk|/it|/ru|lang=*|-en.html|[.]en[.]|en|_en|EN|en-UK/|eng|lang'))\n",
    "    mul_lang1 = soup.find('img',src=re.compile(r'en|usa|uk|it|ru|uk|de|UK|EN|gr|GR|lang|greek|english|EnglishFlag|Flag|flag|lang|language'))\n",
    "    if mul_lang == None and mul_lang1 == None:\n",
    "        mullng_opt = 0\n",
    "    else:\n",
    "        mullng_opt = 1\n",
    "    results.append(mullng_opt)\n",
    "    news_letter1 = soup.find('input',{'name':re.compile(r'[Ee]?mail|EMAIL|e-mail|E-mail')})\n",
    "    news_letter2 = soup.find('img',{'src':re.compile(r'[Nn]ewsletter|NEWSLETTER')})\n",
    "    news_letter3 = soup.find('input',{'placeholder':re.compile(r'[Ee]?mail|EMAIL|e-mail|E-mail|E-mail...')})\n",
    "    news_letter4 = soup.find('input',{'id':re.compile(r'[Ee]?mail|EMAIL|e-mail|E-mail|E-mail...|[nN]ewsletter|NEWSLETTER')})\n",
    "    if news_letter1 != None or news_letter2 != None or news_letter3 !=None or news_letter4 !=None:\n",
    "        newsletter = 1\n",
    "    else:\n",
    "        newsletter = 0\n",
    "    results.append(newsletter)\n",
    "    search_opt1 = soup.find('input',value=re.compile(r'Search|Search this website...|...|Δώστε τίτλο|Αναζήτηση|αναζήτηση|SEARCH ...|Εύρεση|SEARCH|search|ΑΝΑΖΗΤΗΣΗ'))\n",
    "    search_opt2 = soup.find('input',placeholder=re.compile(r'Search|Search this website...|Αναζήτηση|αναζήτηση|SEARCH ...||SEARCH|search|ΑΝΑΖΗΤΗΣΗ'))\n",
    "    search_opt3 = soup.find('input',{'id' : re.compile(r'keyword|searchform|acp_magento_search_id_main_page|searchStr|TextBox|txtSearch|[sS]earch|searchstring')})\n",
    "    search_opt4 = soup.find('input',{'class' : re.compile(r'[S]earch|input|Textbox|assistive-text searchsubmit|searchInput|SEARCH|searchtextfieldlogin|SearchBox|formfieldCopy|search_text|field|go|searchstring')})\n",
    "    if search_opt1 !=None or search_opt2 !=None or search_opt3 !=None or search_opt4!=None:\n",
    "        search = 1\n",
    "    else:\n",
    "        search = 0\n",
    "    results.append(search)\n",
    "    has_blog = soup.find('a',text = re.compile(r'Blog|BLOG|blog'))\n",
    "    has_blog1 = soup.find('a',href = re.compile(r'Blog|BLOG|blog'))\n",
    "    if has_blog !=None or has_blog1!=None:\n",
    "        blog = 1\n",
    "    else:\n",
    "        blog = 0\n",
    "    results.append(blog)\n",
    "    has_app1 = soup.find('img',src=re.compile(r'google-logo|apple-logo|googleplay|appstore'))\n",
    "    has_app2 = soup.find('a',href=re.compile(r'apple.com|https://play.google.com'))\n",
    "    has_app3 = soup.find('a',text=re.compile(r'[aA]pp|APP|[aA]pplication|APPLICATION'))\n",
    "    if has_app1 !=None or has_app2 !=None or has_app3 !=None:\n",
    "        mob_app = 1\n",
    "    else:\n",
    "        mob_app = 0\n",
    "    results.append(mob_app)\n",
    "    results.append(shopping_cart(url))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def social_networks(sp):\n",
    "    import re\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    social_nets = []\n",
    "    social_urls = []\n",
    "    fb1 = sp.find_all('img',src=re.compile('[fF]acebook|fb|FACEBOOK'))\n",
    "    fb2 = sp.find_all('a',href=re.compile(r'[fF]acebook|fb|FACEBOOK'))\n",
    "    fb3 = sp.find_all(text=re.compile(r'[Ff]acebook|FACEBOOK|fb'))\n",
    "    if fb1 !=[] or fb2!= [] or fb3!= []:\n",
    "        if fb1 !=[]:\n",
    "            social_urls.append(fb1)\n",
    "        if fb2 !=[]:\n",
    "            social_urls.append(fb2)\n",
    "        social_nets.append('Facebook')\n",
    "    twt1 = sp.find_all('img',src=re.compile(r'[tT]witter|TWITTER'))\n",
    "    tw2 = sp.find_all('a',href=re.compile(r'[tT]witter|twitter|TWITTER'))\n",
    "    tw3 = sp.find_all(text=re.compile(r'[tT]witter|TWITTER'))\n",
    "    if twt1 !=[] or tw2 !=[] or tw3 !=[]:\n",
    "        if twt1 !=[]:\n",
    "            social_urls.append(twt1)\n",
    "        if tw2!=[]:\n",
    "            social_urls.append(tw2)\n",
    "        social_nets.append('Twitter')\n",
    "    linkedin1 = sp.find_all('a',href=re.compile(r'[Ll]inkedin|LINKEDIN'))\n",
    "    linkedin2 = sp.find_all('a',{'class' :re.compile(r'[Ll]inkedin|LINKEDIN')})\n",
    "    linkedin3 = sp.find_all('img',src=re.compile(r'[Ll]inkedin|LINKEDIN'))\n",
    "    if linkedin1 !=[] or linkedin2 !=[] or linkedin3 !=[]:\n",
    "        if linkedin1 !=[]:\n",
    "            social_urls.append(linkedin1)\n",
    "        if linkedin2 !=[]:\n",
    "            social_urls.append(linkedin2)\n",
    "        if linkedin3 !=[]:\n",
    "            social_urls.append(linkedin3)\n",
    "        social_nets.append('Linkedin')\n",
    "    insta1 = sp.find_all('a',href= re.compile(r'[iI]nstagram|INSTAGRAM'))\n",
    "    insta2 = sp.find_all('img',src= re.compile(r'[iI]nstagram|INSTAGRAM'))\n",
    "    insta3 = sp.find_all('a',{'class' :re.compile(r'[iI]nstagram|INSTAGRAM')})\n",
    "    if insta1 !=[] or insta2!=[] or insta3!=[]:\n",
    "        if insta1 !=[]:\n",
    "            social_urls.append(insta1)\n",
    "        if insta2 !=[]:\n",
    "            social_urls.append(insta2)\n",
    "        if insta3 !=[]:\n",
    "            social_urls.append(insta3)\n",
    "        social_nets.append('Instagram')\n",
    "    yout1 = sp.find_all('a',href= re.compile(r'[yY]outube|YOUTUBE'))\n",
    "    yout2 = sp.find_all('img',src= re.compile(r'[yY]outube|YOUTUBE'))\n",
    "    if yout1 !=[] or yout2 !=[]:\n",
    "        if yout1 !=[]:\n",
    "            social_urls.append(yout1)\n",
    "        if yout2 !=[]:\n",
    "            social_urls.append(yout2)\n",
    "        social_nets.append('Youtube')\n",
    "    s_urls = list(set(x for l in social_urls for x in l))\n",
    "    total_urls = []\n",
    "    for u in s_urls:\n",
    "        if u.has_attr('href'):\n",
    "            total_urls.append(u['href'])\n",
    "        if u.has_attr('src'):\n",
    "            total_urls.append(u['src'])\n",
    "    final = []\n",
    "    for t in total_urls:\n",
    "        if '.jpg' not in t and '.png' not in t:\n",
    "            final.append(t)\n",
    "    return social_nets,final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shopping_cart(url):\n",
    "    import re\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from pprint import pprint\n",
    "    r = requests.get(url)\n",
    "    soup10 = BeautifulSoup(r.content,'lxml')\n",
    "    cart = soup10.find_all('a',href=re.compile(r'[Cc]art|basket|eshop|ESHOP|E-shop|e-shop|E-SHOP|bsk|kalathi|EShop|kala8iagorwn|shop|SHOP|Shop'))\n",
    "    basket = soup10.find_all('div',{'class' :re.compile(r'[Cc]art|basket|shop|eshop|ESHOP|E-shop|E-SHOP|e-shop|bsk|kalathi|EShop|kala8iagorwn')})\n",
    "    if cart !=[] or basket !=[]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_redirects(url):\n",
    "    import requests\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    r1 = requests.get(r.url)\n",
    "    if r.content !=r1.content:\n",
    "        return r.url,r1\n",
    "    else:\n",
    "        return url,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:/Users/User/Desktop/diploma thesis Data Science/the_urls.csv', header = None)\n",
    "urls = list(data[0])\n",
    "del urls[0]\n",
    "urls1 = urls[0:2000]\n",
    "urls2 = urls[2000:len(urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError\n",
    "total1 = []\n",
    "c=0\n",
    "for url in urls1:\n",
    "    c+=1\n",
    "    if c%100 == 0:\n",
    "        print(c)\n",
    "    try:\n",
    "        total1.append(first_crawler(url))\n",
    "    except ConnectionError:\n",
    "        print(url)\n",
    "        continue\n",
    "    except ConnectionResetError:\n",
    "        print(url)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totaldf = pd.DataFrame(total1,columns=['url','social_networks','urls_from_social','multilang_opt','newsletter','search_opt','blog','mob_app','eshop'])\n",
    "totaldf['social_networks'] = totaldf['social_networks'].apply(lambda x: ','.join(x))\n",
    "totaldf['urls_from_social'] = totaldf['urls_from_social'].apply(lambda x: ','.join(x))\n",
    "#totaldf.to_csv('first_crawler_results.csv',encoding='utf-8',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError\n",
    "total2 = []\n",
    "c=0\n",
    "for url in urls2:\n",
    "    c+=1\n",
    "    if c%100 == 0:\n",
    "        print(c)\n",
    "    try:\n",
    "        total2.append(first_crawler(url))\n",
    "    except ConnectionError:\n",
    "        print(url)\n",
    "        continue\n",
    "    except ConnectionResetError:\n",
    "        print(url)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = total1+total2\n",
    "totaldf = pd.DataFrame(total,columns=['url','social_networks','urls_from_social','multilang_opt','newsletter','search_opt','blog','mob_app','eshop'])\n",
    "totaldf['social_networks'] = totaldf['social_networks'].apply(lambda x: ','.join(x))\n",
    "totaldf['urls_from_social'] = totaldf['urls_from_social'].apply(lambda x: ','.join(x))\n",
    "totaldf.to_csv('first_crawler_results.csv',encoding='utf-8',index= False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
